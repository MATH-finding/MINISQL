"""
/tests/test_lexer.py

è¯æ³•åˆ†æžå™¨å•å…ƒæµ‹è¯•
"""
import sys
import os

# å°†ä¸Šçº§ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰æ·»åŠ åˆ° sys.path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from sql.lexer import SQLLexer, TokenType, Token

passed = 0
failed = 0

def assert_test(test_name, condition, message=""):
    global passed, failed
    if condition:
        print(f"âœ… PASS: {test_name}")
        passed += 1
    else:
        print(f"âŒ FAIL: {test_name} - {message}")
        failed += 1

def print_test_summary():
    total = passed + failed
    print("\n" + "=" * 60)
    print(f"ðŸ“Š æµ‹è¯•ç»“æžœç»Ÿè®¡: é€šè¿‡: {passed}  å¤±è´¥: {failed}")
    if total > 0:
        print(f"ðŸ“ˆ é€šè¿‡çŽ‡: {passed / total * 100:.1f}%")
    if failed == 0:
        print("ðŸŽ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")

def test_keywords_and_identifiers():
    sql = "SELECT id, user_name FROM users_table WHERE id = 1;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        expected_tokens = [
            (TokenType.SELECT, 'SELECT'),
            (TokenType.IDENTIFIER, 'id'),
            (TokenType.COMMA, ','),
            (TokenType.IDENTIFIER, 'user_name'),
            (TokenType.FROM, 'FROM'),
            (TokenType.IDENTIFIER, 'users_table'),
            (TokenType.WHERE, 'WHERE'),
            (TokenType.IDENTIFIER, 'id'),
            (TokenType.EQUALS, '='),
            (TokenType.NUMBER, '1'),
            (TokenType.SEMICOLON, ';'),
            (TokenType.EOF, '')
        ]
        cond = len(tokens) == len(expected_tokens) and all(
            tokens[i].type == expected_tokens[i][0] and tokens[i].value == expected_tokens[i][1]
            for i in range(len(tokens))
        )
        assert_test("æµ‹è¯•SQLå…³é”®å­—å’Œæ ‡è¯†ç¬¦çš„è¯†åˆ«", cond)
    except Exception as e:
        assert_test("æµ‹è¯•SQLå…³é”®å­—å’Œæ ‡è¯†ç¬¦çš„è¯†åˆ«", False, str(e))

def test_ddl_statements():
    sql = "CREATE TABLE students (id INTEGER PRIMARY KEY, name VARCHAR(255) NOT NULL);"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        token_types = [t.type for t in tokens]
        expected_types = [
            TokenType.CREATE, TokenType.TABLE, TokenType.IDENTIFIER, TokenType.LEFT_PAREN,
            TokenType.IDENTIFIER, TokenType.INTEGER, TokenType.PRIMARY, TokenType.KEY, TokenType.COMMA,
            TokenType.IDENTIFIER, TokenType.VARCHAR, TokenType.LEFT_PAREN, TokenType.NUMBER, TokenType.RIGHT_PAREN,
            TokenType.NOT, TokenType.NULL, TokenType.RIGHT_PAREN, TokenType.SEMICOLON, TokenType.EOF
        ]
        cond = token_types == expected_types
        assert_test("æµ‹è¯•CREATE/DROP/ALTERç­‰DDLè¯­å¥çš„å…³é”®å­—", cond)
    except Exception as e:
        assert_test("æµ‹è¯•CREATE/DROP/ALTERç­‰DDLè¯­å¥çš„å…³é”®å­—", False, str(e))

def test_dml_statements():
    sql = "INSERT INTO t (c1) VALUES ('hello'); UPDATE t SET c1 = 'world'; DELETE FROM t;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        token_types = [t.type for t in tokens]
        expected_types = [
            TokenType.INSERT, TokenType.INTO, TokenType.IDENTIFIER, TokenType.LEFT_PAREN, TokenType.IDENTIFIER, 
            TokenType.RIGHT_PAREN, TokenType.VALUES, TokenType.LEFT_PAREN, TokenType.STRING, TokenType.RIGHT_PAREN, 
            TokenType.SEMICOLON,
            TokenType.UPDATE, TokenType.IDENTIFIER, TokenType.SET, TokenType.IDENTIFIER, TokenType.EQUALS, 
            TokenType.STRING, TokenType.SEMICOLON,
            TokenType.DELETE, TokenType.FROM, TokenType.IDENTIFIER, TokenType.SEMICOLON,
            TokenType.EOF
        ]
        cond = token_types == expected_types
        assert_test("æµ‹è¯•INSERT/UPDATE/DELETEç­‰DMLè¯­å¥çš„å…³é”®å­—", cond)
    except Exception as e:
        assert_test("æµ‹è¯•INSERT/UPDATE/DELETEç­‰DMLè¯­å¥çš„å…³é”®å­—", False, str(e))

def test_operators_and_separators():
    sql = "a > b, c < d, e >= f, g <= h, i != j, k = l.m * (n);"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        token_values = [t.value for t in tokens if t.type != TokenType.IDENTIFIER]
        expected_values = [
            '>', ',', '<', ',', '>=', ',', '<=', ',', '!=', ',', '=', '.', '*', '(', ')', ';', ''
        ]
        cond = token_values == expected_values
        assert_test("æµ‹è¯•æ‰€æœ‰è¿ç®—ç¬¦å’Œåˆ†éš”ç¬¦", cond)
    except Exception as e:
        assert_test("æµ‹è¯•æ‰€æœ‰è¿ç®—ç¬¦å’Œåˆ†éš”ç¬¦", False, str(e))

def test_literals():
    sql = "123, 45.67, 'a string', \"another string\", -99;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        expected = [
            (TokenType.NUMBER, '123'),
            (TokenType.COMMA, ','),
            (TokenType.NUMBER, '45.67'),
            (TokenType.COMMA, ','),
            (TokenType.STRING, 'a string'),
            (TokenType.COMMA, ','),
            (TokenType.STRING, 'another string'),
            (TokenType.COMMA, ','),
            (TokenType.NUMBER, '-99'),
            (TokenType.SEMICOLON, ';'),
            (TokenType.EOF, '')
        ]
        cond = len(tokens) == len(expected) and all(
            tokens[i].type == expected[i][0] and tokens[i].value == expected[i][1]
            for i in range(len(tokens))
        )
        assert_test("æµ‹è¯•å­—ç¬¦ä¸²ã€æ•°å­—ç­‰å¸¸é‡", cond)
    except Exception as e:
        assert_test("æµ‹è¯•å­—ç¬¦ä¸²ã€æ•°å­—ç­‰å¸¸é‡", False, str(e))

def test_line_and_column_tracking():
    sql = "SELECT\n  id\nFROM users;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        select_token = tokens[0]
        id_token = tokens[1]
        from_token = tokens[2]
        cond = (
            select_token.line == 1 and select_token.column == 1 and
            id_token.line == 2 and id_token.column == 3 and  # ä¸¤ä¸ªç©ºæ ¼+id
            from_token.line == 3 and from_token.column == 1
        )
        assert_test("æµ‹è¯•è¡Œå·å’Œåˆ—å·çš„è·Ÿè¸ª", cond)
    except Exception as e:
        assert_test("æµ‹è¯•è¡Œå·å’Œåˆ—å·çš„è·Ÿè¸ª", False, str(e))

def test_illegal_character_error():
    sql = "SELECT id FROM users WHERE id @ 1;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        assert_test("æµ‹è¯•éžæ³•å­—ç¬¦é”™è¯¯æç¤º", False, "Expected SyntaxError not raised")
    except SyntaxError as e:
        line = getattr(e, 'line', '?')
        column = getattr(e, 'column', '?')
        assert_test("æµ‹è¯•éžæ³•å­—ç¬¦é”™è¯¯æç¤º", True, f"Expected position: {line}, {column}")

def test_unclosed_string_error():
    sql = "SELECT 'hello world;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        assert_test("æµ‹è¯•æœªé—­åˆçš„å­—ç¬¦ä¸²é”™è¯¯", False, "Expected SyntaxError not raised")
    except SyntaxError as e:
        line = getattr(e, 'line', '?')
        column = getattr(e, 'column', '?')
        assert_test("æµ‹è¯•æœªé—­åˆçš„å­—ç¬¦ä¸²é”™è¯¯", True, f"Expected position: {line}, {column}")

def test_all_transaction_keywords():
    sql = "BEGIN TRANSACTION; COMMIT; ROLLBACK; SET AUTOCOMMIT=0; SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;"
    lexer = SQLLexer(sql)
    try:
        tokens = lexer.tokenize()
        for token in tokens:
            print(str(token))
        token_types = [t.type for t in tokens]
        expected_types = [
            TokenType.BEGIN, TokenType.TRANSACTION, TokenType.SEMICOLON,
            TokenType.COMMIT, TokenType.SEMICOLON,
            TokenType.ROLLBACK, TokenType.SEMICOLON,
            TokenType.SET, TokenType.AUTOCOMMIT, TokenType.EQUALS, TokenType.NUMBER, TokenType.SEMICOLON,
            TokenType.SET, TokenType.SESSION, TokenType.TRANSACTION, TokenType.ISOLATION, TokenType.LEVEL, 
            TokenType.REPEATABLE, TokenType.READ, TokenType.SEMICOLON,
            TokenType.EOF
        ]
        cond = token_types == expected_types
        assert_test("æµ‹è¯•æ‰€æœ‰äº‹åŠ¡ç›¸å…³çš„å…³é”®å­—", cond)
    except Exception as e:
        assert_test("æµ‹è¯•æ‰€æœ‰äº‹åŠ¡ç›¸å…³çš„å…³é”®å­—", False, str(e))

def main():
    test_keywords_and_identifiers()
    test_ddl_statements()
    test_dml_statements()
    test_operators_and_separators()
    test_literals()
    test_line_and_column_tracking()
    test_illegal_character_error()
    test_unclosed_string_error()
    test_all_transaction_keywords()
    print_test_summary()

if __name__ == "__main__":
    main()